{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbe70a96",
   "metadata": {},
   "source": [
    "## Interval Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbda12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorboardX\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "# from tensorboardX import SummaryWriter\n",
    "\n",
    "use_cuda = False\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "batch_size = 64\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "## Dataloaders\n",
    "train_dataset = datasets.MNIST('mnist_data/', train=True, download=True, transform=transforms.Compose(\n",
    "    [transforms.ToTensor()]\n",
    "))\n",
    "test_dataset = datasets.MNIST('mnist_data/', train=False, download=True, transform=transforms.Compose(\n",
    "    [transforms.ToTensor()]\n",
    "))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "## Simple NN. You can change this if you want. If you change it, mention the architectural details in your report.\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc = nn.Linear(28*28, 200)\n",
    "        self.fc2 = nn.Linear(200,10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view((-1, 28*28))\n",
    "        x = F.relu(self.fc(x))\n",
    "        x = self.fc2(x)\n",
    "        x = F.softmax(x, dim=-1) # added softmax for probabilities\n",
    "        return x\n",
    "\n",
    "class Normalize(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return (x - 0.1307)/0.3081\n",
    "\n",
    "# Add the data normalization as a first \"layer\" to the network\n",
    "# this allows us to search for adverserial examples to the real image, rather than\n",
    "# to the normalized image\n",
    "model = nn.Sequential(Normalize(), Net())\n",
    "\n",
    "model = model.to(device)\n",
    "model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6282f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, num_epochs):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader):.3f}')\n",
    "\n",
    "def test_model(model):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        print(f'Accuracy on images: {100 * correct / total}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dd8bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model, 15)\n",
    "test_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c23f92",
   "metadata": {},
   "source": [
    "### Write the interval analysis for the simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97f7f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Write the interval analysis for the simple model\n",
    "## you can use https://github.com/Zinoex/bound_propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620dbfd0",
   "metadata": {},
   "source": [
    "First, we define some tool functions for bound propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e46d7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# l=lower bound, u=upper bound\n",
    "def interval_affine(l, u, W, b):\n",
    "    W_pos = torch.clamp(W, min=0)\n",
    "    W_neg = torch.clamp(W, max=0)\n",
    "    y_l = W_pos @ l + W_neg @ u + b\n",
    "    y_u = W_pos @ u + W_neg @ l + b\n",
    "    return y_l, y_u\n",
    "\n",
    "def interval_relu(l, u):\n",
    "    zl = torch.clamp(l, min=0.0)\n",
    "    zu = torch.clamp(u, min=0.0)\n",
    "    return zl, zu\n",
    "\n",
    "def interval_normalize(l, u, mean=0.1307, std=0.3081):\n",
    "    return (l - mean) / std, (u - mean) / std\n",
    "\n",
    "# x is a concrete input, so the bounds are also concrete, we can easily propagate them.\n",
    "# We propagate [x-eps, x+eps] through Normalize -> Linear -> ReLU -> Linear.\n",
    "def get_logits_bounds(model, x, eps):\n",
    "    l0 = torch.clamp(x - eps, 0.0, 1.0).view(-1)\n",
    "    u0 = torch.clamp(x + eps, 0.0, 1.0).view(-1)\n",
    "\n",
    "    l1, u1 = interval_normalize(l0, u0, mean=0.1307, std=0.3081)\n",
    "\n",
    "    W1 = model[1].fc.weight.detach() # [200, 784]\n",
    "    b1 = model[1].fc.bias.detach()   # [200]\n",
    "    l2, u2 = interval_affine(l1, u1, W1, b1)\n",
    "\n",
    "    l3, u3 = interval_relu(l2, u2)\n",
    "\n",
    "    W2 = model[1].fc2.weight.detach() # [10, 200]\n",
    "    b2 = model[1].fc2.bias.detach()   # [10]\n",
    "    l4, u4 = interval_affine(l3, u3, W2, b2) # logits bounds\n",
    "\n",
    "    return l4, u4  #shape: [10], [10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0b0bbe",
   "metadata": {},
   "source": [
    "Then, we define the function to do the verification for a single input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204c8456",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_output_logits(model, x): # We can skip the softmax here.\n",
    "    normalize = model[0] # Normalize()\n",
    "    net = model[1]       # Net() with .fc and .fc2\n",
    "    x = normalize(x)\n",
    "    x = x.view(x.size(0), -1)\n",
    "    x = F.relu(net.fc(x))\n",
    "    logits = net.fc2(x)\n",
    "    return logits\n",
    "\n",
    "# Given eps, verify concrete sample x\n",
    "@torch.no_grad()\n",
    "def verify_sample(model, x, y_label, eps):\n",
    "    logits = get_output_logits(model, x.unsqueeze(0)).squeeze(0)\n",
    "    pred = int(torch.argmax(logits).item())\n",
    "    correct = (pred == int(y_label.item()))\n",
    "\n",
    "    l_logit, u_logit = get_logits_bounds(model, x.squeeze(0), eps)\n",
    "\n",
    "    # certification condition: for all j != c, lower_bound(logit_c - logit_j) > 0\n",
    "    if correct:\n",
    "        c = pred\n",
    "        margins = l_logit[c] - u_logit  # vector of size 10\n",
    "        margins[c] = torch.tensor(float('inf'))  # we ignore j=c\n",
    "        verified = torch.min(margins) > 0\n",
    "    else:\n",
    "        verified = False\n",
    "\n",
    "    return bool(verified)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be328243",
   "metadata": {},
   "source": [
    "Use the functions above, we can implement all the analysis in one function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fed513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate verified accuracy on the whole test set for 10 epsilons\n",
    "@torch.no_grad()\n",
    "def evaluate_verified_accuracy(model, test_loader, epsilons):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    results = {eps: {'verified': 0, 'eligible': 0} for eps in epsilons}\n",
    "    # eligible = number of samples that are correctly classified without perturbation\n",
    "\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch = images.size(0)\n",
    "        total += batch\n",
    "\n",
    "        logits = get_output_logits(model, images)\n",
    "        preds = logits.argmax(dim=1)\n",
    "\n",
    "        # per-sample verification for each eps\n",
    "        for i in range(batch):\n",
    "            x = images[i:i+1].cpu()\n",
    "            y = labels[i:i+1].cpu()\n",
    "            clean_ok = (preds[i].item() == y.item())\n",
    "            for eps in epsilons:\n",
    "                if clean_ok:\n",
    "                    results[eps]['eligible'] += 1\n",
    "                    v = verify_sample(model.cpu(), x, y, eps)\n",
    "                    if v:\n",
    "                        results[eps]['verified'] += 1\n",
    "\n",
    "    verified_acc = {eps: (100.0 * results[eps]['verified'] / max(1, results[eps]['eligible']))\n",
    "                    for eps in epsilons}\n",
    "    return verified_acc, results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9e0562",
   "metadata": {},
   "source": [
    "## Run experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b6fb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation\n",
    "eps_list = np.linspace(0.01, 0.1, 10).tolist()\n",
    "verified_acc, raw = evaluate_verified_accuracy(model, test_loader, eps_list)\n",
    "print(\"[Verified Accuracy over eps]:\")\n",
    "for eps in eps_list:\n",
    "    print(f\"  eps={eps:.3f}: {verified_acc[eps]:.2f}%  \"\n",
    "          f\"(verified {raw[eps]['verified']}/{raw[eps]['eligible']} eligible)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs521",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
